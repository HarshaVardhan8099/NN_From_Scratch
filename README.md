# NN_From_Scratch
Neural network implemented from scratch in Python
# Neural Network on MNIST (from Scratch with CuPy)

This repository contains a simple, from-scratch implementation of a feedforward neural network. The key feature of this project is its dynamic backend, which automatically uses **CuPy for GPU acceleration** if a CUDA-enabled device is available, and falls back to **NumPy for CPU-based computation** if not.

The network is designed to be **modular and easy to understand**, with a clear separation of concerns for different components like layers, activation functions, and loss functions. It is trained and evaluated on the classic **MNIST handwritten digits** dataset.

âš¡ No deep learning frameworks (TensorFlow, PyTorch, Keras, etc.) were used.

---

## ðŸš€ Key Features

- **Adaptive Backend**: Seamlessly switches between CuPy (for GPU) and NumPy (for CPU) to optimize performance based on your system.
- **From-Scratch Implementation**: All core components, including dense layers, activations, and backpropagation, are built from the ground up.
- **Modular Design**: The codebase is organized into distinct files, making it easy to understand and extend.
- **MNIST Dataset**: Includes a data loader to handle fetching, preprocessing, and splitting the MNIST dataset.
- **Model Persistence**: The trained model can be saved to a file and loaded later for inference.

## ðŸ“ Project Structure

```text
.
â”œâ”€â”€ main.py           # Main script for training the model
â”œâ”€â”€ model.py          # Defines the SimpleNN class and model flow
â”œâ”€â”€ layers.py         # Implements Dense (fully connected) layers
â”œâ”€â”€ activations.py    # ReLU, Softmax, and other activations
â”œâ”€â”€ losses.py         # CrossEntropy loss function
â”œâ”€â”€ data_loader.py    # MNIST data loading and preprocessing
â”œâ”€â”€ backend.py        # Dynamic backend selector (NumPy or CuPy)
â”œâ”€â”€ utils.py          # Utility functions (e.g., accuracy calculation)
â”œâ”€â”€ test_model.py     # Load trained model and visualize predictions
â””â”€â”€ nn_mnist.pkl      # Saved model file (after training)

```
---

## âœ… Prerequisites

- Python 3.7 or higher
- A CUDA-enabled GPU (optional, for GPU acceleration with CuPy)

## ðŸ”§ Installation

1. **Create a virtual environment (recommended)**:
    ```bash
    python -m venv venv
    source venv/bin/activate  # or venv\Scripts\activate on Windows
    ```

2. **Install dependencies**:
    ```bash
    pip install numpy scikit-learn matplotlib
    ```

3. **(Optional) For GPU acceleration with CuPy**:
    Only if you have a CUDA-enabled GPU:
    ```bash
    pip install cupy-cuda11x  # Replace 11x with your CUDA version
    ```

> âš ï¸ Note: Avoid installing CuPy in environments like GitHub Codespaces or Google Colab unless you're sure a compatible GPU is available.


## âš™ï¸ How the CPU/GPU Handling Works (`backend.py`)

The `backend.py` file handles automatic selection of the backend:

- **Main Variable (`xp`)**: Acts as an alias for either `numpy` or `cupy`.
- **Logic**:
  - Detects if running in a restricted cloud environment (e.g., Codespace).
  - Tries to import CuPy.
  - Checks for CUDA-compatible GPU.

**Result**:
- If GPU is available, `xp = cupy`.
- Otherwise, `xp = numpy`.

> This abstraction means the rest of the code doesnâ€™t care whether it's running on a CPU or GPU â€” just use `xp` for all numerical operations.

## ðŸ§  Usage

### 1. Train and test the Model

    ```bash
    python main.py

    python test_model.py
    ```

---

## ðŸ“œ License

This project is open-source and free to use under the [MIT License](LICENSE).

---

## ðŸ™Œ Acknowledgements

- [MNIST dataset](http://yann.lecun.com/exdb/mnist/)
- NumPy, CuPy, Matplotlib, scikit-learn

---

